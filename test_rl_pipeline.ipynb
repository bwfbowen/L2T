{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random \n",
    "import numpy as np\n",
    "import torch \n",
    "from stable_baselines3 import PPO, DQN, HerReplayBuffer\n",
    "from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env.vec_monitor import VecMonitor\n",
    "\n",
    "from src.env import MultiODEnv, SparseMultiODEnv\n",
    "from src.solution import MultiODSolution\n",
    "from src.problem import MultiODProblem\n",
    "from src.utils import read_instance_data, get_lkh3_tour, get_ortools_tour\n",
    "from src.rl.stable_baselines3.nn import PSExtractor\n",
    "from src.rl.stable_baselines3.callback import SaveBestSolCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_dir = os.path.join('data', 'tsppdlib', 'instances', 'random-uniform')\n",
    "instances = [i for i in os.listdir(instance_dir) if i.endswith('.tsp')]\n",
    "num_Os = [\"005\", \"010\", \"020\", \"050\", \"100\"]\n",
    "num_O = '050'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 random instances for each num_O, with random.sample(sub_instances, k=5)\n",
    "sub_instances = [i for i in instances if '-' + num_O + '-' in i]\n",
    "resample = False  \n",
    "\n",
    "if resample:\n",
    "    sub_instances = random.sample(sub_instances, k=5)\n",
    "else:\n",
    "    if num_O == \"005\": \n",
    "        sub_instances = [\n",
    "            'random-005-06203.tsp',\n",
    "            'random-005-14680.tsp',\n",
    "            'random-005-27025.tsp',\n",
    "            'random-005-22010.tsp',\n",
    "            'random-005-27053.tsp']\n",
    "    elif num_O == \"010\":\n",
    "        sub_instances = [\n",
    "            'random-010-05876.tsp',\n",
    "            'random-010-13200.tsp',\n",
    "            'random-010-07248.tsp',\n",
    "            'random-010-11763.tsp',\n",
    "            'random-010-20971.tsp']\n",
    "    elif num_O == \"020\":\n",
    "        sub_instances = [\n",
    "            'random-020-13151.tsp',\n",
    "            'random-020-32388.tsp',\n",
    "            'random-020-19723.tsp',\n",
    "            'random-020-02593.tsp',\n",
    "            'random-020-10770.tsp']\n",
    "    elif num_O == \"050\":\n",
    "        sub_instances = [\n",
    "            # 'random-050-13219.tsp',\n",
    "            # 'random-050-29393.tsp',\n",
    "            # 'random-050-04371.tsp',\n",
    "            # 'random-050-12086.tsp',\n",
    "            'random-050-21722.tsp']\n",
    "    elif num_O == \"100\":\n",
    "        sub_instances = [\n",
    "            'random-100-19642.tsp',\n",
    "            'random-100-00562.tsp',\n",
    "            'random-100-17825.tsp',\n",
    "            'random-100-18734.tsp',\n",
    "            'random-100-26486.tsp']\n",
    "    else:\n",
    "        sub_instances = random.sample(sub_instances, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lkh3_dir = os.path.join('/home/fangbowen/', 'U')\n",
    "lkh3_results = os.listdir(lkh3_dir)\n",
    "\n",
    "ortools_dir = os.path.join('/home/fangbowen/', 'tmp', 'ortools')\n",
    "ortools_results = os.listdir(ortools_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "hidden_dim = 256\n",
    "num_heads = 16\n",
    "lr = 0.001\n",
    "net_arch = [256, 256]\n",
    "batch_size = 1000\n",
    "\n",
    "# env\n",
    "episode_max_time_length = int(1e3)\n",
    "episode_max_length = int(1e4)\n",
    "n_steps = episode_max_length\n",
    "learn_totoal_steps = int(5e2) * episode_max_length\n",
    "k_recent = 5\n",
    "nenv = 5\n",
    "\n",
    "# callback\n",
    "verbose = 1\n",
    "early_stop = True\n",
    "tensorboard_log = '../tmp/ppo'\n",
    "callback_log_dir = '../tmp/paths'\n",
    "\n",
    "use_sparse_reward = False \n",
    "use_her = False \n",
    "\n",
    "# HER\n",
    "n_sampled_goal = 4\n",
    "goal_selection_strategy = 'future'\n",
    "replay_buffer_kwargs=dict(n_sampled_goal=n_sampled_goal, goal_selection_strategy=goal_selection_strategy)\n",
    "\n",
    "# action_dict\n",
    "use_naive_action = False \n",
    "\n",
    "def get_naive_action_dict(env_instance):\n",
    "    _actions = [ \n",
    "               'actions.PathAction({idx}, operator=operators.ExchangeOperator())',\n",
    "               'actions.PathAction({idx}, operator=operators.InsertOperator())',\n",
    "               ]\n",
    "    _action_dict = {idx: eval(_action.format(idx=idx)) for idx, _action in enumerate(_actions, start=1)}\n",
    "    _action_dict[0] = env_instance._regenerate_feasible_solution\n",
    "    return _action_dict\n",
    "action_dict = None if not use_naive_action else get_naive_action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class MultiODEnvMaker:\n",
    "    def __init__(self, problem, action_dict, max_length, max_time_length, k_recent):\n",
    "        self.problem = problem\n",
    "        self.action_dict = action_dict\n",
    "        self.max_length = max_length\n",
    "        self.max_time_length = max_time_length\n",
    "        self.k_recent = k_recent\n",
    "\n",
    "    def __call__(self):\n",
    "        problem = deepcopy(self.problem)\n",
    "        env = MultiODEnv(problem=problem, \n",
    "                         action_dict=self.action_dict,\n",
    "                         max_length=self.max_length, \n",
    "                         max_time_length=self.max_time_length,\n",
    "                         k_recent=self.k_recent)\n",
    "        return env\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_instances)):\n",
    "    instance_name = sub_instances[i]\n",
    "    instance_name_head = instance_name[:instance_name.index('.tsp')]\n",
    "    print(f'instance: {instance_name_head}')\n",
    "    \n",
    "    lkh3_instance_result = [i for i in lkh3_results if instance_name_head in i][0]\n",
    "    lkh3_tour = get_lkh3_tour(os.path.join(lkh3_dir, lkh3_instance_result))\n",
    "    ortools_instance_result = [i for i in ortools_results if instance_name_head in i][0]\n",
    "    ortools_tour = get_ortools_tour(os.path.join(ortools_dir, ortools_instance_result))\n",
    "    \n",
    "    instance =  os.path.join(instance_dir, instance_name)\n",
    "    locations = read_instance_data(instance)\n",
    "    problem = MultiODProblem(locations=locations, ignore_to_dummy_cost=False, ignore_from_dummy_cost=False)\n",
    "    problem.convert_distance_matrix_to_int()\n",
    "\n",
    "    lkh3_tour = MultiODSolution([lkh3_tour], problem)\n",
    "    ortools_tour = MultiODSolution([ortools_tour], problem)\n",
    "    lkh3_cost, ortools_cost = problem.calc_cost(lkh3_tour), problem.calc_cost(ortools_tour)\n",
    "    print(f'LKH3 cost: {lkh3_cost}, ortools cost: {ortools_cost}')\n",
    "    if lkh3_cost < ortools_cost:\n",
    "        target_tour = lkh3_tour  \n",
    "        print('Target tour is LKH3')\n",
    "    else:\n",
    "        target_tour = ortools_tour\n",
    "        print('Target tour is ortools')\n",
    "    \n",
    "    if use_sparse_reward:\n",
    "        env = VecMonitor(SubprocVecEnv([SparseMultiODEnv(target_cost=int(problem.calc_cost(target_tour) * (1 + 0.05)), \n",
    "                               problem=problem, \n",
    "                               action_dict=action_dict,\n",
    "                               max_length=episode_max_length, \n",
    "                               max_time_length=episode_max_time_length,\n",
    "                               k_recent=k_recent) for _ in range(nenv)]))\n",
    "    else:\n",
    "        env = VecMonitor(SubprocVecEnv([MultiODEnvMaker(problem, action_dict, episode_max_length, episode_max_time_length, k_recent) for _ in range(nenv)]))\n",
    "    \n",
    "    features_dim = env.observation_space['solution'].shape[-1] + env.observation_space['problem'].shape[0]\n",
    "    \n",
    "    policy_kwargs = dict(\n",
    "        features_extractor_class=PSExtractor,\n",
    "        features_extractor_kwargs=dict(features_dim=features_dim, \n",
    "                                    sol_input_dim=env.observation_space['solution'].shape[-1],\n",
    "                                    hidden_dim=hidden_dim,\n",
    "                                    num_heads=num_heads),\n",
    "        net_arch=net_arch\n",
    "    )\n",
    "    \n",
    "    if use_her:\n",
    "        model = DQN(\"MultiInputPolicy\", env, policy_kwargs=policy_kwargs, verbose=verbose,\n",
    "                train_freq=n_steps, batch_size=batch_size, tensorboard_log=tensorboard_log,\n",
    "                replay_buffer_class=HerReplayBuffer, replay_buffer_kwargs=replay_buffer_kwargs\n",
    "                )\n",
    "    else:\n",
    "        model = PPO(\"MultiInputPolicy\", env, policy_kwargs=policy_kwargs, verbose=verbose, n_steps=n_steps, batch_size=batch_size, learning_rate=lr, tensorboard_log=tensorboard_log)\n",
    "    \n",
    "    instance_save_as = instance_name[:instance_name.index('.tsp')]\n",
    "    model.learn(learn_totoal_steps, \n",
    "                tb_log_name=instance_save_as,\n",
    "                callback=SaveBestSolCallback(log_dir=callback_log_dir, \n",
    "                                            instance_name=instance_save_as, \n",
    "                                            verbose=verbose,\n",
    "                                            target_tour=target_tour,\n",
    "                                            early_stop=early_stop)\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch_length = int(4e4)\n",
    "instance_name = 'random-050-13219.tsp'\n",
    "instance_name_head = instance_name[:instance_name.index('.tsp')]\n",
    "instance =  os.path.join(instance_dir, instance_name)\n",
    "locations = read_instance_data(instance)\n",
    "problem = MultiODProblem(locations=locations, ignore_to_dummy_cost=False, ignore_from_dummy_cost=False)\n",
    "problem.convert_distance_matrix_to_int()\n",
    "\n",
    "if use_sparse_reward:\n",
    "    env = SparseMultiODEnv(target_cost=int(problem.calc_cost(target_tour) * (1 + 0.05)), \n",
    "                               problem=problem, \n",
    "                               max_length=episode_max_length, \n",
    "                               max_time_length=episode_max_time_length,\n",
    "                               k_recent=k_recent)\n",
    "else:\n",
    "    env = MultiODEnv(problem=problem, \n",
    "                         max_length=episode_max_length, \n",
    "                         max_time_length=episode_max_time_length,\n",
    "                         k_recent=k_recent)\n",
    "\n",
    "saved_best_model = sorted([i for i in os.listdir(callback_log_dir) if instance_name_head in i and 'model' in i], key=lambda x: int(x[len(instance_name_head) + 1: x.index('.model')]))[0]\n",
    "\n",
    "if use_her:\n",
    "    model = DQN.load(os.path.join(callback_log_dir, saved_best_model), print_system_info=True)\n",
    "else:\n",
    "    model = PPO.load(os.path.join(callback_log_dir, saved_best_model), print_system_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "best_cost = np.inf \n",
    "obs, info = env.reset()\n",
    "for _ in tqdm(range(test_epoch_length)):\n",
    "    action, _states = model.predict(obs) \n",
    "    obs, reward, terminated, truncated, info = env.step(int(action))\n",
    "    if terminated or truncated:\n",
    "        best_cost = min(best_cost, env.best_cost)\n",
    "        obs, info = env.reset() \n",
    "print(best_cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
