{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rlor38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import random \n",
    "import numpy as np\n",
    "import torch \n",
    "from stable_baselines3 import PPO, DQN, HerReplayBuffer\n",
    "from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env.vec_monitor import VecMonitor\n",
    "\n",
    "from src.env import MultiODEnv, SparseMultiODEnv\n",
    "from src import actions \n",
    "from src.solution import MultiODSolution\n",
    "from src.problem import MultiODProblem\n",
    "from src.utils import read_instance_data, get_lkh3_tour, get_ortools_tour\n",
    "from src.rl.stable_baselines3.nn import PSExtractor\n",
    "from src.rl.stable_baselines3.callback import SaveBestSolCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_dir = os.path.join('data', 'tsppdlib', 'instances', 'random-uniform')\n",
    "instances = [i for i in os.listdir(instance_dir) if i.endswith('.tsp')]\n",
    "num_Os = [\"005\", \"010\", \"020\", \"050\", \"100\", \"200\"]\n",
    "num_O = '200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 random instances for each num_O, with random.sample(sub_instances, k=5)\n",
    "sub_instances = [i for i in instances if '-' + num_O + '-' in i]\n",
    "resample = False  \n",
    "\n",
    "if resample:\n",
    "    sub_instances = random.sample(sub_instances, k=5)\n",
    "else:\n",
    "    if num_O == \"005\": \n",
    "        sub_instances = [\n",
    "            'random-005-06203.tsp',\n",
    "            'random-005-14680.tsp',\n",
    "            'random-005-27025.tsp',\n",
    "            'random-005-22010.tsp',\n",
    "            'random-005-27053.tsp']\n",
    "    elif num_O == \"010\":\n",
    "        sub_instances = [\n",
    "            'random-010-05876.tsp',\n",
    "            'random-010-13200.tsp',\n",
    "            'random-010-07248.tsp',\n",
    "            'random-010-11763.tsp',\n",
    "            'random-010-20971.tsp']\n",
    "    elif num_O == \"020\":\n",
    "        sub_instances = [\n",
    "            'random-020-13151.tsp',\n",
    "            'random-020-32388.tsp',\n",
    "            'random-020-19723.tsp',\n",
    "            'random-020-02593.tsp',\n",
    "            'random-020-10770.tsp']\n",
    "    elif num_O == \"050\":\n",
    "        sub_instances = [\n",
    "            'random-050-13219.tsp',\n",
    "            'random-050-29393.tsp',\n",
    "            'random-050-04371.tsp',\n",
    "            'random-050-12086.tsp',\n",
    "            'random-050-21722.tsp']\n",
    "    elif num_O == \"100\":\n",
    "        sub_instances = [\n",
    "            'random-100-19642.tsp',\n",
    "            'random-100-00562.tsp',\n",
    "            'random-100-17825.tsp',\n",
    "            'random-100-18734.tsp',\n",
    "            'random-100-26486.tsp']\n",
    "    elif num_O == \"200\":\n",
    "        sub_instances = [\n",
    "            'random-200-02527.tsp',\n",
    "            'random-200-04236.tsp',\n",
    "            'random-200-04282.tsp',\n",
    "            'random-200-13517.tsp', \n",
    "            'random-200-16462.tsp']\n",
    "    else:\n",
    "        sub_instances = random.sample(sub_instances, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lkh3_dir = os.path.join('/home/fangbowen/', 'U')\n",
    "lkh3_results = os.listdir(lkh3_dir)\n",
    "\n",
    "ortools_dir = os.path.join('/home/fangbowen/', 'tmp', 'ortools')\n",
    "ortools_results = os.listdir(ortools_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "hidden_dim = 256\n",
    "num_heads = 16\n",
    "lr = 0.001\n",
    "net_arch = [256, 256]\n",
    "batch_size = 1000\n",
    "\n",
    "# env\n",
    "episode_max_time_length = int(1e3)\n",
    "episode_max_length = int(1e4)\n",
    "n_steps = episode_max_length\n",
    "n_gradient_steps = 50\n",
    "learn_totoal_steps = int(5e2) * episode_max_length\n",
    "k_recent = 5\n",
    "nenv = 1\n",
    "\n",
    "# callback\n",
    "verbose = 1\n",
    "early_stop = True\n",
    "tensorboard_log = '../tmp/ppo'\n",
    "callback_log_dir = '../tmp/paths'\n",
    "\n",
    "use_sparse_reward = False \n",
    "use_her = False \n",
    "\n",
    "# HER\n",
    "n_sampled_goal = 4\n",
    "goal_selection_strategy = 'future'\n",
    "replay_buffer_kwargs=dict(n_sampled_goal=n_sampled_goal, goal_selection_strategy=goal_selection_strategy)\n",
    "\n",
    "# action_dict\n",
    "use_naive_action = False \n",
    "\n",
    "def get_naive_action_dict(env_instance):\n",
    "    _actions = [ \n",
    "               'actions.PathAction({idx}, operator=operators.ExchangeOperator())',\n",
    "               'actions.PathAction({idx}, operator=operators.InsertOperator())',\n",
    "               ]\n",
    "    _action_dict = {idx: eval(_action.format(idx=idx)) for idx, _action in enumerate(_actions, start=1)}\n",
    "    _action_dict[0] = env_instance._regenerate_feasible_solution\n",
    "    return _action_dict\n",
    "action_dict = None if not use_naive_action else get_naive_action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class MultiODEnvMaker:\n",
    "    def __init__(self, problem, action_dict, max_length, max_time_length, k_recent):\n",
    "        self.problem = problem\n",
    "        self.action_dict = action_dict\n",
    "        self.max_length = max_length\n",
    "        self.max_time_length = max_time_length\n",
    "        self.k_recent = k_recent\n",
    "\n",
    "    def __call__(self):\n",
    "        problem = deepcopy(self.problem)\n",
    "        env = MultiODEnv(problem=problem, \n",
    "                         action_dict=self.action_dict,\n",
    "                         max_length=self.max_length, \n",
    "                         max_time_length=self.max_time_length,\n",
    "                         k_recent=self.k_recent)\n",
    "        return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance: random-100-19642\n",
      "LKH3 cost: 12514.0, ortools cost: 14183.0\n",
      "Target tour is LKH3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rlor38/lib/python3.8/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ../tmp/ppo/random-100-19642_3\n",
      "Target cost: 12514\n",
      "Best solution cost: 73164, found at 1 step, 2.35 seconds used\n",
      "Best solution cost: 19182, found at 2 step, 19.21 seconds used\n",
      "Best solution cost: 18941, found at 4 step, 19.24 seconds used\n",
      "Best solution cost: 18416, found at 5 step, 19.81 seconds used\n",
      "Best solution cost: 17102, found at 22 step, 30.36 seconds used\n",
      "Best solution cost: 16953, found at 32 step, 35.31 seconds used\n",
      "Best solution cost: 16777, found at 68 step, 65.30 seconds used\n",
      "Best solution cost: 16740, found at 71 step, 65.33 seconds used\n",
      "Best solution cost: 16561, found at 106 step, 75.14 seconds used\n",
      "Best solution cost: 16465, found at 121 step, 83.68 seconds used\n",
      "Best solution cost: 16395, found at 124 step, 83.77 seconds used\n",
      "Best solution cost: 16351, found at 127 step, 83.97 seconds used\n",
      "Best solution cost: 16236, found at 132 step, 84.02 seconds used\n",
      "Best solution cost: 16084, found at 135 step, 84.72 seconds used\n",
      "Best solution cost: 15969, found at 174 step, 101.43 seconds used\n",
      "Best solution cost: 15938, found at 176 step, 101.51 seconds used\n",
      "Best solution cost: 15631, found at 182 step, 101.58 seconds used\n",
      "Best solution cost: 15121, found at 186 step, 102.60 seconds used\n",
      "Best solution cost: 14935, found at 281 step, 141.45 seconds used\n",
      "Best solution cost: 14910, found at 499 step, 217.09 seconds used\n",
      "Best solution cost: 14909, found at 501 step, 217.18 seconds used\n",
      "Best solution cost: 14904, found at 664 step, 272.30 seconds used\n",
      "Best solution cost: 14788, found at 1007 step, 384.79 seconds used\n",
      "Best solution cost: 14775, found at 1011 step, 384.96 seconds used\n",
      "Best solution cost: 14647, found at 1173 step, 436.72 seconds used\n",
      "Best solution cost: 14633, found at 1175 step, 436.82 seconds used\n",
      "Best solution cost: 14517, found at 1504 step, 544.66 seconds used\n",
      "Best solution cost: 14479, found at 1510 step, 544.79 seconds used\n",
      "Best solution cost: 14389, found at 1718 step, 610.21 seconds used\n",
      "Best solution cost: 14371, found at 2386 step, 844.45 seconds used\n",
      "Best solution cost: 14256, found at 2580 step, 921.55 seconds used\n",
      "Best solution cost: 14220, found at 2585 step, 921.81 seconds used\n",
      "Best solution cost: 14190, found at 2589 step, 922.10 seconds used\n",
      "Best solution cost: 14148, found at 2602 step, 933.43 seconds used\n",
      "Best solution cost: 14146, found at 2603 step, 933.45 seconds used\n",
      "Best solution cost: 13929, found at 2604 step, 1907.75 seconds used\n",
      "Best solution cost: 13892, found at 2609 step, 1907.93 seconds used\n",
      "Best solution cost: 13874, found at 2708 step, 1939.95 seconds used\n",
      "Rollout best solution cost: 13874, \n",
      "                  found at 2708 step, 1939.65 seconds used. \n",
      "                  Convergence gap: 13874.0. Target gap: 1360\n",
      "---------------------------------------------\n",
      "| best/                          |          |\n",
      "|    best_cost                   | 13874    |\n",
      "|    best_sol_at_step            | 2708     |\n",
      "|    best_sol_found_time         | 1.94e+03 |\n",
      "| rollout/                       |          |\n",
      "|    convergence_gap             | 1.39e+04 |\n",
      "|    ep_len_mean                 | 2.85e+03 |\n",
      "|    ep_rew_mean                 | 8.66e+04 |\n",
      "|    rollout_best_cost           | 13874    |\n",
      "|    rollout_best_sol_at_step    | 2708     |\n",
      "|    rollout_best_sol_found_time | 1.94e+03 |\n",
      "|    target_gap                  | 1360     |\n",
      "| target/                        |          |\n",
      "|    target_cost                 | 12514    |\n",
      "| time/                          |          |\n",
      "|    fps                         | 2        |\n",
      "|    iterations                  | 1        |\n",
      "|    time_elapsed                | 3469     |\n",
      "|    total_timesteps             | 10000    |\n",
      "---------------------------------------------\n",
      "Best solution cost: 13738, found at 1919 step, 3934.73 seconds used\n",
      "Best solution cost: 13658, found at 1963 step, 3953.91 seconds used\n",
      "Rollout best solution cost: 13658, \n",
      "                  found at 1963 step, 203.03 seconds used. \n",
      "                  Convergence gap: 216. Target gap: 1144\n",
      "------------------------------------------------\n",
      "| best/                          |             |\n",
      "|    best_cost                   | 13658       |\n",
      "|    best_sol_at_step            | 1963        |\n",
      "|    best_sol_found_time         | 3.95e+03    |\n",
      "| rollout/                       |             |\n",
      "|    convergence_gap             | 216         |\n",
      "|    ep_len_mean                 | 2.76e+03    |\n",
      "|    ep_rew_mean                 | 8.92e+04    |\n",
      "|    rollout_best_cost           | 13658       |\n",
      "|    rollout_best_sol_at_step    | 1963        |\n",
      "|    rollout_best_sol_found_time | 203         |\n",
      "|    target_gap                  | 1144        |\n",
      "| time/                          |             |\n",
      "|    fps                         | 2           |\n",
      "|    iterations                  | 2           |\n",
      "|    time_elapsed                | 7250        |\n",
      "|    total_timesteps             | 20000       |\n",
      "| train/                         |             |\n",
      "|    approx_kl                   | 0.001395568 |\n",
      "|    clip_fraction               | 0           |\n",
      "|    clip_range                  | 0.2         |\n",
      "|    entropy_loss                | -2.48       |\n",
      "|    explained_variance          | -1.92e-05   |\n",
      "|    learning_rate               | 0.001       |\n",
      "|    loss                        | 4.68e+07    |\n",
      "|    n_updates                   | 50          |\n",
      "|    policy_gradient_loss        | -0.00297    |\n",
      "|    value_loss                  | 9.88e+07    |\n",
      "------------------------------------------------\n",
      "Best solution cost: 13639, found at 1321 step, 8497.99 seconds used\n",
      "Best solution cost: 13632, found at 2045 step, 8753.41 seconds used\n",
      "Best solution cost: 13589, found at 2093 step, 8768.12 seconds used\n",
      "Rollout best solution cost: 13589, \n",
      "                  found at 2093 step, 1233.44 seconds used. \n",
      "                  Convergence gap: 69. Target gap: 1075\n",
      "-------------------------------------------------\n",
      "| best/                          |              |\n",
      "|    best_cost                   | 13589        |\n",
      "|    best_sol_at_step            | 2093         |\n",
      "|    best_sol_found_time         | 8.77e+03     |\n",
      "| rollout/                       |              |\n",
      "|    convergence_gap             | 69           |\n",
      "|    ep_len_mean                 | 2.71e+03     |\n",
      "|    ep_rew_mean                 | 8.99e+04     |\n",
      "|    rollout_best_cost           | 13589        |\n",
      "|    rollout_best_sol_at_step    | 2093         |\n",
      "|    rollout_best_sol_found_time | 1.23e+03     |\n",
      "|    target_gap                  | 1075         |\n",
      "| time/                          |              |\n",
      "|    fps                         | 2            |\n",
      "|    iterations                  | 3            |\n",
      "|    time_elapsed                | 11088        |\n",
      "|    total_timesteps             | 30000        |\n",
      "| train/                         |              |\n",
      "|    approx_kl                   | 0.0008194993 |\n",
      "|    clip_fraction               | 2e-06        |\n",
      "|    clip_range                  | 0.2          |\n",
      "|    entropy_loss                | -2.48        |\n",
      "|    explained_variance          | 0.0195       |\n",
      "|    learning_rate               | 0.001        |\n",
      "|    loss                        | 4.45e+07     |\n",
      "|    n_updates                   | 100          |\n",
      "|    policy_gradient_loss        | -0.00197     |\n",
      "|    value_loss                  | 9.48e+07     |\n",
      "-------------------------------------------------\n",
      "Rollout best solution cost: 13625, \n",
      "                  found at 2581 step, 1586.57 seconds used. \n",
      "                  Convergence gap: 36. Target gap: 1111\n",
      "--------------------------------------------------\n",
      "| rollout/                       |               |\n",
      "|    convergence_gap             | 36            |\n",
      "|    ep_len_mean                 | 2.68e+03      |\n",
      "|    ep_rew_mean                 | 9e+04         |\n",
      "|    rollout_best_cost           | 13625         |\n",
      "|    rollout_best_sol_at_step    | 2581          |\n",
      "|    rollout_best_sol_found_time | 1.59e+03      |\n",
      "|    target_gap                  | 1111          |\n",
      "| time/                          |               |\n",
      "|    fps                         | 2             |\n",
      "|    iterations                  | 4             |\n",
      "|    time_elapsed                | 14901         |\n",
      "|    total_timesteps             | 40000         |\n",
      "| train/                         |               |\n",
      "|    approx_kl                   | 0.00029808615 |\n",
      "|    clip_fraction               | 3.8e-05       |\n",
      "|    clip_range                  | 0.2           |\n",
      "|    entropy_loss                | -2.48         |\n",
      "|    explained_variance          | 0.0359        |\n",
      "|    learning_rate               | 0.001         |\n",
      "|    loss                        | 4.43e+07      |\n",
      "|    n_updates                   | 150           |\n",
      "|    policy_gradient_loss        | -0.000981     |\n",
      "|    value_loss                  | 9.23e+07      |\n",
      "--------------------------------------------------\n",
      "Best solution cost: 13525, found at 1559 step, 15768.27 seconds used\n",
      "Rollout best solution cost: 13525, \n",
      "                  found at 1559 step, 587.48 seconds used. \n",
      "                  Convergence gap: 100. Target gap: 1011\n",
      "--------------------------------------------------\n",
      "| best/                          |               |\n",
      "|    best_cost                   | 13525         |\n",
      "|    best_sol_at_step            | 1559          |\n",
      "|    best_sol_found_time         | 1.58e+04      |\n",
      "| rollout/                       |               |\n",
      "|    convergence_gap             | 100           |\n",
      "|    ep_len_mean                 | 2.72e+03      |\n",
      "|    ep_rew_mean                 | 8.83e+04      |\n",
      "|    rollout_best_cost           | 13525         |\n",
      "|    rollout_best_sol_at_step    | 1559          |\n",
      "|    rollout_best_sol_found_time | 587           |\n",
      "|    target_gap                  | 1011          |\n",
      "| time/                          |               |\n",
      "|    fps                         | 2             |\n",
      "|    iterations                  | 5             |\n",
      "|    time_elapsed                | 18524         |\n",
      "|    total_timesteps             | 50000         |\n",
      "| train/                         |               |\n",
      "|    approx_kl                   | 0.00046398304 |\n",
      "|    clip_fraction               | 2e-06         |\n",
      "|    clip_range                  | 0.2           |\n",
      "|    entropy_loss                | -2.48         |\n",
      "|    explained_variance          | 0.0492        |\n",
      "|    learning_rate               | 0.001         |\n",
      "|    loss                        | 4.41e+07      |\n",
      "|    n_updates                   | 200           |\n",
      "|    policy_gradient_loss        | -0.00111      |\n",
      "|    value_loss                  | 8.79e+07      |\n",
      "--------------------------------------------------\n",
      "Rollout best solution cost: 14206, \n",
      "                  found at 2025 step, 371.60 seconds used. \n",
      "                  Convergence gap: 681. Target gap: 1692\n",
      "--------------------------------------------------\n",
      "| rollout/                       |               |\n",
      "|    convergence_gap             | 681           |\n",
      "|    ep_len_mean                 | 2.72e+03      |\n",
      "|    ep_rew_mean                 | 8.82e+04      |\n",
      "|    rollout_best_cost           | 14206         |\n",
      "|    rollout_best_sol_at_step    | 2025          |\n",
      "|    rollout_best_sol_found_time | 372           |\n",
      "|    target_gap                  | 1692          |\n",
      "| time/                          |               |\n",
      "|    fps                         | 2             |\n",
      "|    iterations                  | 6             |\n",
      "|    time_elapsed                | 22238         |\n",
      "|    total_timesteps             | 60000         |\n",
      "| train/                         |               |\n",
      "|    approx_kl                   | 0.00042973884 |\n",
      "|    clip_fraction               | 0.000158      |\n",
      "|    clip_range                  | 0.2           |\n",
      "|    entropy_loss                | -2.48         |\n",
      "|    explained_variance          | 0.0634        |\n",
      "|    learning_rate               | 0.001         |\n",
      "|    loss                        | 4.02e+07      |\n",
      "|    n_updates                   | 250           |\n",
      "|    policy_gradient_loss        | -0.00104      |\n",
      "|    value_loss                  | 8.5e+07       |\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m     model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39m\u001b[39mMultiInputPolicy\u001b[39m\u001b[39m\"\u001b[39m, env, policy_kwargs\u001b[39m=\u001b[39mpolicy_kwargs, verbose\u001b[39m=\u001b[39mverbose, n_steps\u001b[39m=\u001b[39mn_steps, batch_size\u001b[39m=\u001b[39mbatch_size, learning_rate\u001b[39m=\u001b[39mlr, n_epochs\u001b[39m=\u001b[39mn_gradient_steps, tensorboard_log\u001b[39m=\u001b[39mtensorboard_log)\n\u001b[1;32m     57\u001b[0m instance_save_as \u001b[39m=\u001b[39m instance_name[:instance_name\u001b[39m.\u001b[39mindex(\u001b[39m'\u001b[39m\u001b[39m.tsp\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m---> 58\u001b[0m model\u001b[39m.\u001b[39;49mlearn(learn_totoal_steps, \n\u001b[1;32m     59\u001b[0m             tb_log_name\u001b[39m=\u001b[39;49minstance_save_as,\n\u001b[1;32m     60\u001b[0m             callback\u001b[39m=\u001b[39;49mSaveBestSolCallback(log_dir\u001b[39m=\u001b[39;49mcallback_log_dir, \n\u001b[1;32m     61\u001b[0m                                         instance_name\u001b[39m=\u001b[39;49minstance_save_as, \n\u001b[1;32m     62\u001b[0m                                         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m     63\u001b[0m                                         target_tour\u001b[39m=\u001b[39;49mtarget_tour,\n\u001b[1;32m     64\u001b[0m                                         early_stop\u001b[39m=\u001b[39;49mearly_stop)\n\u001b[1;32m     65\u001b[0m             )\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m    176\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[0;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    182\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     59\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/LAHR/src/env.py:96\u001b[0m, in \u001b[0;36mMultiODEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     95\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_improvement \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msolution, all_delta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_dict[action](\u001b[39mself\u001b[39;49m)\n\u001b[1;32m     97\u001b[0m \u001b[39m# no improvement:\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39mif\u001b[39;00m action \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m all_delta \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39mEPSILON:\n",
      "File \u001b[0;32m~/LAHR/src/actions.py:60\u001b[0m, in \u001b[0;36mPathAction.__call__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     58\u001b[0m modified \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \n\u001b[1;32m     59\u001b[0m \u001b[39mwhile\u001b[39;00m modified:\n\u001b[0;32m---> 60\u001b[0m     improved_path, delta, modified \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update(improved_solution, path_id)\n\u001b[1;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m modified:\n\u001b[1;32m     63\u001b[0m         all_delta \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m delta \n",
      "File \u001b[0;32m~/LAHR/src/actions.py:67\u001b[0m, in \u001b[0;36mPathAction._update\u001b[0;34m(self, solution, path_id)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update\u001b[39m(\u001b[39mself\u001b[39m, solution: MultiODSolution, path_id: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     improved_path, delta, label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moperator(solution\u001b[39m=\u001b[39;49msolution, path_id\u001b[39m=\u001b[39;49mpath_id)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# print(label)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     modified \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m label \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/LAHR/src/operators.py:132\u001b[0m, in \u001b[0;36mExchangeOperator.__call__\u001b[0;34m(self, solution, path_id, min_delta)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m od1 \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    131\u001b[0m     d: Node \u001b[39m=\u001b[39m path\u001b[39m.\u001b[39mget_by_node_id(path\u001b[39m.\u001b[39mOD_mapping[node1\u001b[39m.\u001b[39mnode_id])\n\u001b[0;32m--> 132\u001b[0m     inner_min_delta, inner_label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inner_loop(first, node1, path, first\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m, d\u001b[39m.\u001b[39;49mseq_id, min_delta)\n\u001b[1;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     inner_min_delta, inner_label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_loop(first, node1, path, first\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, n, min_delta)\n",
      "File \u001b[0;32m~/LAHR/src/operators.py:173\u001b[0m, in \u001b[0;36mExchangeOperator._inner_loop\u001b[0;34m(self, first, node1, path, start, end, min_delta)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     before \u001b[39m=\u001b[39m (\n\u001b[1;32m    166\u001b[0m         path\u001b[39m.\u001b[39mget_distance_by_node_ids(prev1, node1\u001b[39m.\u001b[39mnode_id)\n\u001b[1;32m    167\u001b[0m         \u001b[39m+\u001b[39m path\u001b[39m.\u001b[39mget_distance_by_node_ids(node1\u001b[39m.\u001b[39mnode_id, next1)\n\u001b[1;32m    168\u001b[0m         \u001b[39m+\u001b[39m path\u001b[39m.\u001b[39mget_distance_by_node_ids(prev2, node2\u001b[39m.\u001b[39mnode_id)\n\u001b[1;32m    169\u001b[0m         \u001b[39m+\u001b[39m path\u001b[39m.\u001b[39mget_distance_by_node_ids(node2\u001b[39m.\u001b[39mnode_id, next2)\n\u001b[1;32m    170\u001b[0m         )\n\u001b[1;32m    171\u001b[0m     after \u001b[39m=\u001b[39m (\n\u001b[1;32m    172\u001b[0m         path\u001b[39m.\u001b[39mget_distance_by_node_ids(prev1, node2\u001b[39m.\u001b[39mnode_id)\n\u001b[0;32m--> 173\u001b[0m         \u001b[39m+\u001b[39m path\u001b[39m.\u001b[39;49mget_distance_by_node_ids(node2\u001b[39m.\u001b[39;49mnode_id, next1)\n\u001b[1;32m    174\u001b[0m         \u001b[39m+\u001b[39m path\u001b[39m.\u001b[39mget_distance_by_node_ids(prev2, node1\u001b[39m.\u001b[39mnode_id)\n\u001b[1;32m    175\u001b[0m         \u001b[39m+\u001b[39m path\u001b[39m.\u001b[39mget_distance_by_node_ids(node1\u001b[39m.\u001b[39mnode_id, next2)\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    177\u001b[0m delta \u001b[39m=\u001b[39m after \u001b[39m-\u001b[39m before\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m delta \u001b[39m<\u001b[39m min_delta:\n",
      "File \u001b[0;32m~/LAHR/src/solution.py:127\u001b[0m, in \u001b[0;36mMultiODPath.get_distance_by_node_ids\u001b[0;34m(self, node_id1, node_id2)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_distance_by_node_ids\u001b[39m(\u001b[39mself\u001b[39m, node_id1: \u001b[39mint\u001b[39m, node_id2: \u001b[39mint\u001b[39m):\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistance_matrix[node_id1, node_id2]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(sub_instances)):\n",
    "    instance_name = sub_instances[i]\n",
    "    instance_name_head = instance_name[:instance_name.index('.tsp')]\n",
    "    print(f'instance: {instance_name_head}')\n",
    "    \n",
    "    lkh3_instance_result = [i for i in lkh3_results if instance_name_head in i][0]\n",
    "    lkh3_tour = get_lkh3_tour(os.path.join(lkh3_dir, lkh3_instance_result))\n",
    "    ortools_instance_result = [i for i in ortools_results if instance_name_head in i][0]\n",
    "    ortools_tour = get_ortools_tour(os.path.join(ortools_dir, ortools_instance_result))\n",
    "    \n",
    "    instance =  os.path.join(instance_dir, instance_name)\n",
    "    locations = read_instance_data(instance)\n",
    "    problem = MultiODProblem(locations=locations, ignore_to_dummy_cost=False, ignore_from_dummy_cost=False)\n",
    "    problem.convert_distance_matrix_to_int()\n",
    "\n",
    "    lkh3_tour = MultiODSolution([lkh3_tour], problem)\n",
    "    ortools_tour = MultiODSolution([ortools_tour], problem)\n",
    "    lkh3_cost, ortools_cost = problem.calc_cost(lkh3_tour), problem.calc_cost(ortools_tour)\n",
    "    print(f'LKH3 cost: {lkh3_cost}, ortools cost: {ortools_cost}')\n",
    "    if lkh3_cost < ortools_cost:\n",
    "        target_tour = lkh3_tour  \n",
    "        print('Target tour is LKH3')\n",
    "    else:\n",
    "        target_tour = ortools_tour\n",
    "        print('Target tour is ortools')\n",
    "    \n",
    "    if use_sparse_reward:\n",
    "        env = SparseMultiODEnv(target_cost=int(problem.calc_cost(target_tour) * (1 + 0.05)), \n",
    "                               problem=problem, \n",
    "                               action_dict=action_dict,\n",
    "                               max_length=episode_max_length, \n",
    "                               max_time_length=episode_max_time_length,\n",
    "                               k_recent=k_recent)\n",
    "    else:\n",
    "        # env = VecMonitor(SubprocVecEnv([MultiODEnvMaker(problem, action_dict, episode_max_length, episode_max_time_length, k_recent) for _ in range(nenv)]))\n",
    "        env = MultiODEnv(problem=problem, action_dict=action_dict, max_length=episode_max_length, max_time_length=episode_max_time_length, k_recent=k_recent)\n",
    "    \n",
    "    features_dim = env.observation_space['solution'].shape[-1] + env.observation_space['problem'].shape[0]\n",
    "    \n",
    "    policy_kwargs = dict(\n",
    "        features_extractor_class=PSExtractor,\n",
    "        features_extractor_kwargs=dict(features_dim=features_dim, \n",
    "                                    sol_input_dim=env.observation_space['solution'].shape[-1],\n",
    "                                    hidden_dim=hidden_dim,\n",
    "                                    num_heads=num_heads),\n",
    "        net_arch=net_arch\n",
    "    )\n",
    "    \n",
    "    if use_her:\n",
    "        model = DQN(\"MultiInputPolicy\", env, policy_kwargs=policy_kwargs, verbose=verbose,\n",
    "                train_freq=n_steps, batch_size=batch_size, tensorboard_log=tensorboard_log,\n",
    "                replay_buffer_class=HerReplayBuffer, replay_buffer_kwargs=replay_buffer_kwargs\n",
    "                )\n",
    "    else:\n",
    "        model = PPO(\"MultiInputPolicy\", env, policy_kwargs=policy_kwargs, verbose=verbose, n_steps=n_steps, batch_size=batch_size, learning_rate=lr, n_epochs=n_gradient_steps, tensorboard_log=tensorboard_log)\n",
    "    \n",
    "    instance_save_as = instance_name[:instance_name.index('.tsp')]\n",
    "    model.learn(learn_totoal_steps, \n",
    "                tb_log_name=instance_save_as,\n",
    "                callback=SaveBestSolCallback(log_dir=callback_log_dir, \n",
    "                                            instance_name=instance_save_as, \n",
    "                                            verbose=verbose,\n",
    "                                            target_tour=target_tour,\n",
    "                                            early_stop=early_stop)\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch_length = int(4e4)\n",
    "instance_name = 'random-050-13219.tsp'\n",
    "instance_name_head = instance_name[:instance_name.index('.tsp')]\n",
    "instance =  os.path.join(instance_dir, instance_name)\n",
    "locations = read_instance_data(instance)\n",
    "problem = MultiODProblem(locations=locations, ignore_to_dummy_cost=False, ignore_from_dummy_cost=False)\n",
    "problem.convert_distance_matrix_to_int()\n",
    "\n",
    "if use_sparse_reward:\n",
    "    env = SparseMultiODEnv(target_cost=int(problem.calc_cost(target_tour) * (1 + 0.05)), \n",
    "                               problem=problem, \n",
    "                               max_length=episode_max_length, \n",
    "                               max_time_length=episode_max_time_length,\n",
    "                               k_recent=k_recent)\n",
    "else:\n",
    "    env = MultiODEnv(problem=problem, \n",
    "                         max_length=episode_max_length, \n",
    "                         max_time_length=episode_max_time_length,\n",
    "                         k_recent=k_recent)\n",
    "\n",
    "saved_best_model = sorted([i for i in os.listdir(callback_log_dir) if instance_name_head in i and 'model' in i], key=lambda x: int(x[len(instance_name_head) + 1: x.index('.model')]))[0]\n",
    "\n",
    "if use_her:\n",
    "    model = DQN.load(os.path.join(callback_log_dir, saved_best_model), print_system_info=True)\n",
    "else:\n",
    "    model = PPO.load(os.path.join(callback_log_dir, saved_best_model), print_system_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "best_cost = np.inf \n",
    "obs, info = env.reset()\n",
    "for _ in tqdm(range(test_epoch_length)):\n",
    "    action, _states = model.predict(obs) \n",
    "    obs, reward, terminated, truncated, info = env.step(int(action))\n",
    "    if terminated or truncated:\n",
    "        best_cost = min(best_cost, env.best_cost)\n",
    "        obs, info = env.reset() \n",
    "print(best_cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
