{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/miniconda3/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "/usr/local/anaconda3/envs/miniconda3/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  ) < LooseVersion(\"1.15\"):\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import torch \n",
    "from stable_baselines3 import PPO, DQN, HerReplayBuffer\n",
    "from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy\n",
    "\n",
    "from src.env import MultiODEnv, SparseMultiODEnv\n",
    "from src.solution import MultiODSolution\n",
    "from src.problem import MultiODProblem\n",
    "from src.utils import read_instance_data, get_lkh3_tour\n",
    "from src.rl.stable_baselines3.nn import PSExtractor\n",
    "from src.rl.stable_baselines3.callback import SaveBestSolCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_dir = os.path.join('data', 'tsppdlib', 'instances', 'random-uniform')\n",
    "instances = [i for i in os.listdir(instance_dir) if i.endswith('.tsp')]\n",
    "num_Os = [\"005\", \"010\", \"020\", \"050\"]\n",
    "num_O = '020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_max_time_length = int(1e3)\n",
    "episode_max_length = int(4e3)\n",
    "n_steps = episode_max_length\n",
    "learn_totoal_steps = int(5e2) * episode_max_length\n",
    "verbose = 1\n",
    "batch_size = 100\n",
    "tensorboard_log = '../tmp/ppo'\n",
    "callback_log_dir = '../tmp/paths'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_instances = [i for i in instances if '-' + num_O + '-' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lkh3_dir = os.path.join('/home/fangbowen/', 'U')\n",
    "lkh3_results = os.listdir(lkh3_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing num-005:   0%|                                | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance: random-020-05654\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ../tmp/ppo/random-005-03942_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rlor38/lib/python3.8/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ../tmp/ppo/random-020-05654_7\n",
      "Target cost: 6287\n",
      "Best solution cost: 18928, found at 1 step, 1.74 seconds used\n",
      "Best solution cost: 17276, found at 2 step, 1.74 seconds used\n",
      "Best solution cost: 16454, found at 3 step, 1.75 seconds used\n",
      "Best solution cost: 14284, found at 4 step, 1.75 seconds used\n",
      "Best solution cost: 10343, found at 7 step, 1.77 seconds used\n",
      "Best solution cost: 7974, found at 16 step, 1.86 seconds used\n",
      "Best solution cost: 7941, found at 20 step, 1.88 seconds used\n",
      "Best solution cost: 7715, found at 50 step, 2.16 seconds used\n",
      "Best solution cost: 7704, found at 53 step, 2.18 seconds used\n",
      "Best solution cost: 6989, found at 69 step, 2.36 seconds used\n",
      "Best solution cost: 6893, found at 283 step, 4.07 seconds used\n",
      "Best solution cost: 6609, found at 296 step, 4.16 seconds used\n",
      "Best solution cost: 6599, found at 2058 step, 17.23 seconds used\n",
      "Rollout best solution cost: 6599, \n",
      "                  found at 2058 step, 17.15 seconds used. \n",
      "                  Convergence gap: 6599.0. Target gap: 312\n",
      "---------------------------------------------\n",
      "| best/                          |          |\n",
      "|    best_cost                   | 6599     |\n",
      "|    best_sol_at_step            | 2058     |\n",
      "|    best_sol_found_time         | 17.2     |\n",
      "| rollout/                       |          |\n",
      "|    convergence_gap             | 6.6e+03  |\n",
      "|    ep_len_mean                 | 4e+03    |\n",
      "|    ep_rew_mean                 | 5.54e+04 |\n",
      "|    rollout_best_cost           | 6599     |\n",
      "|    rollout_best_sol_at_step    | 2058     |\n",
      "|    rollout_best_sol_found_time | 17.2     |\n",
      "|    target_gap                  | 312      |\n",
      "| time/                          |          |\n",
      "|    fps                         | 129      |\n",
      "|    iterations                  | 1        |\n",
      "|    time_elapsed                | 30       |\n",
      "|    total_timesteps             | 4000     |\n",
      "---------------------------------------------\n",
      "Best solution cost: 6287, found at 1349 step, 45.77 seconds used\n",
      "Rollout best solution cost: 6287, \n",
      "                  found at 1349 step, 10.51 seconds used. \n",
      "                  Convergence gap: 312. Target gap: 0\n",
      "-------------------------------------------------\n",
      "| best/                          |              |\n",
      "|    best_cost                   | 6287         |\n",
      "|    best_sol_at_step            | 1349         |\n",
      "|    best_sol_found_time         | 45.8         |\n",
      "| rollout/                       |              |\n",
      "|    convergence_gap             | 312          |\n",
      "|    rollout_best_cost           | 6287         |\n",
      "|    rollout_best_sol_at_step    | 1349         |\n",
      "|    rollout_best_sol_found_time | 10.5         |\n",
      "|    target_gap                  | 0            |\n",
      "| train/                         |              |\n",
      "|    approx_kl                   | 0.0009435665 |\n",
      "|    clip_fraction               | 0            |\n",
      "|    clip_range                  | 0.2          |\n",
      "|    entropy_loss                | -2.71        |\n",
      "|    explained_variance          | 0.000116     |\n",
      "|    learning_rate               | 0.001        |\n",
      "|    loss                        | 9.14e+06     |\n",
      "|    n_updates                   | 10           |\n",
      "|    policy_gradient_loss        | -0.00284     |\n",
      "|    value_loss                  | 2.11e+07     |\n",
      "-------------------------------------------------\n",
      "instance: random-020-29583\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ../tmp/ppo/random-020-29583_6\n",
      "Target cost: 5902\n",
      "Best solution cost: 20313, found at 1 step, 0.07 seconds used\n",
      "Best solution cost: 18525, found at 2 step, 0.08 seconds used\n",
      "Best solution cost: 18337, found at 3 step, 0.08 seconds used\n",
      "Best solution cost: 14038, found at 4 step, 0.08 seconds used\n",
      "Best solution cost: 8709, found at 7 step, 0.10 seconds used\n",
      "Best solution cost: 8446, found at 8 step, 0.10 seconds used\n",
      "Best solution cost: 8071, found at 19 step, 0.17 seconds used\n",
      "Best solution cost: 6495, found at 26 step, 0.28 seconds used\n",
      "Best solution cost: 6441, found at 88 step, 0.86 seconds used\n",
      "Best solution cost: 6388, found at 283 step, 2.26 seconds used\n",
      "Best solution cost: 6345, found at 284 step, 2.27 seconds used\n",
      "Best solution cost: 6283, found at 374 step, 3.04 seconds used\n",
      "Best solution cost: 6094, found at 1102 step, 8.47 seconds used\n",
      "Best solution cost: 6092, found at 3310 step, 24.55 seconds used\n",
      "Best solution cost: 6008, found at 3336 step, 24.68 seconds used\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m# model = DQN(\"MultiInputPolicy\", env, policy_kwargs=policy_kwargs, verbose=verbose,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m#             train_freq=n_steps, batch_size=batch_size, tensorboard_log=tensorboard_log,\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m#             replay_buffer_class=HerReplayBuffer, replay_buffer_kwargs=replay_buffer_kwargs\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m#             )\u001b[39;00m\n\u001b[1;32m     34\u001b[0m instance_save_as \u001b[39m=\u001b[39m instance_name[:instance_name\u001b[39m.\u001b[39mindex(\u001b[39m'\u001b[39m\u001b[39m.tsp\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m---> 35\u001b[0m model\u001b[39m.\u001b[39;49mlearn(learn_totoal_steps, \n\u001b[1;32m     36\u001b[0m             tb_log_name\u001b[39m=\u001b[39;49minstance_save_as,\n\u001b[1;32m     37\u001b[0m             callback\u001b[39m=\u001b[39;49mSaveBestSolCallback(log_dir\u001b[39m=\u001b[39;49mcallback_log_dir, \n\u001b[1;32m     38\u001b[0m                                         instance_name\u001b[39m=\u001b[39;49minstance_save_as, \n\u001b[1;32m     39\u001b[0m                                         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m     40\u001b[0m                                         target_tour\u001b[39m=\u001b[39;49mlkh3_tour)\n\u001b[1;32m     41\u001b[0m             )\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:169\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     obs_tensor \u001b[39m=\u001b[39m obs_as_tensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 169\u001b[0m     actions, values, log_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(obs_tensor)\n\u001b[1;32m    170\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    172\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/stable_baselines3/common/policies.py:617\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[39mForward pass in all the networks (actor and critic)\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[39m:return: action, value and log probability of the action\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[39m# Preprocess the observation if needed\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_features(obs)\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshare_features_extractor:\n\u001b[1;32m    619\u001b[0m     latent_pi, latent_vf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp_extractor(features)\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/stable_baselines3/common/policies.py:640\u001b[0m, in \u001b[0;36mActorCriticPolicy.extract_features\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[39mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \n\u001b[1;32m    636\u001b[0m \u001b[39m:param obs: Observation\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[39m:return: the output of the features extractor(s)\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshare_features_extractor:\n\u001b[0;32m--> 640\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mextract_features(obs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures_extractor)\n\u001b[1;32m    641\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    642\u001b[0m     pi_features \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mextract_features(obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpi_features_extractor)\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/stable_baselines3/common/policies.py:131\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[0;34m(self, obs, features_extractor)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39m :return: The extracted features\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m preprocessed_obs \u001b[39m=\u001b[39m preprocess_obs(obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space, normalize_images\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_images)\n\u001b[0;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m features_extractor(preprocessed_obs)\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/LAHR/src/rl/stable_baselines3/nn.py:36\u001b[0m, in \u001b[0;36mPSExtractor.forward\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m     34\u001b[0m sol_embed \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtranspose(sol_embed, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m     35\u001b[0m identity \u001b[39m=\u001b[39m sol_embed\n\u001b[0;32m---> 36\u001b[0m sol_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msol_residual(sol_embed)\n\u001b[1;32m     37\u001b[0m sol_out \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m identity\n\u001b[1;32m     38\u001b[0m sol_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msol_norm_after(sol_out)\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/torch/nn/modules/conv.py:307\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/opt/conda/envs/rlor38/lib/python3.8/site-packages/torch/nn/modules/conv.py:303\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    301\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    302\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 303\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    304\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(sub_instances)):\n",
    "    instance_name = sub_instances[i]\n",
    "    instance_name_head = instance_name[:instance_name.index('.tsp')]\n",
    "    print(f'instance: {instance_name_head}')\n",
    "    lkh3_instance_result = [i for i in lkh3_results if instance_name_head in i][0]\n",
    "    lkh3_tour = get_lkh3_tour(os.path.join(lkh3_dir, lkh3_instance_result))\n",
    "    instance = f'/home/fangbowen/LAHR/data/tsppdlib/instances/random-uniform/{instance_name}'\n",
    "    locations = read_instance_data(instance)\n",
    "    problem = MultiODProblem(locations=locations, ignore_to_dummy_cost=False)\n",
    "    problem.convert_distance_matrix_to_int()\n",
    "    lkh3_tour = MultiODSolution([lkh3_tour], problem)\n",
    "    env = MultiODEnv(problem=problem, max_length=episode_max_length, max_time_length=episode_max_time_length)\n",
    "    # env = SparseMultiODEnv(target_cost=int(target_cost * (1 + 0.05)), problem=problem, max_length=episode_max_length, max_time_length=episode_max_time_length)\n",
    "    features_dim = env.observation_space['solution'].shape[-1] + env.observation_space['problem'].shape[0]\n",
    "    hidden_dim = 64\n",
    "    num_heads = 4\n",
    "    lr = 0.001\n",
    "\n",
    "    policy_kwargs = dict(\n",
    "        features_extractor_class=PSExtractor,\n",
    "        features_extractor_kwargs=dict(features_dim=features_dim, \n",
    "                                    sol_input_dim=env.observation_space['solution'].shape[-1],\n",
    "                                    hidden_dim=hidden_dim,\n",
    "                                    num_heads=num_heads),\n",
    "        net_arch=[256, 256]\n",
    "    )\n",
    "    goal_selection_strategy = 'future'\n",
    "    replay_buffer_kwargs=dict(n_sampled_goal=4, goal_selection_strategy=goal_selection_strategy)\n",
    "    model = PPO(\"MultiInputPolicy\", env, policy_kwargs=policy_kwargs, verbose=verbose, n_steps=n_steps, batch_size=batch_size, learning_rate=lr, tensorboard_log=tensorboard_log)\n",
    "    # model = DQN(\"MultiInputPolicy\", env, policy_kwargs=policy_kwargs, verbose=verbose,\n",
    "    #             train_freq=n_steps, batch_size=batch_size, tensorboard_log=tensorboard_log,\n",
    "    #             replay_buffer_class=HerReplayBuffer, replay_buffer_kwargs=replay_buffer_kwargs\n",
    "    #             )\n",
    "    instance_save_as = instance_name[:instance_name.index('.tsp')]\n",
    "    model.learn(learn_totoal_steps, \n",
    "                tb_log_name=instance_save_as,\n",
    "                callback=SaveBestSolCallback(log_dir=callback_log_dir, \n",
    "                                            instance_name=instance_save_as, \n",
    "                                            verbose=verbose,\n",
    "                                            target_tour=lkh3_tour)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch_length = int(4e4)\n",
    "\n",
    "\n",
    "for _ in range(test_epoch_length):\n",
    "    pass "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
