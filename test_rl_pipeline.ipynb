{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import torch \n",
    "from stable_baselines3 import PPO, DQN\n",
    "\n",
    "from src.env import MultiODEnv, SparseMultiODEnv\n",
    "from src.problem import MultiODProblem\n",
    "from src.utils import read_instance_data\n",
    "from src.rl.stable_baselines3.nn import PSExtractor\n",
    "from src.rl.stable_baselines3.callback import SaveBestSolCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_dir = os.path.join('data', 'tsppdlib', 'instances', 'random-uniform')\n",
    "instances = [i for i in os.listdir(instance_dir) if i.endswith('.tsp')]\n",
    "num_Os = [\"005\", \"010\", \"020\", \"050\"]\n",
    "num_O = '050'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_max_time_length = int(1e3)\n",
    "episode_max_length = int(2e4)\n",
    "n_steps = episode_max_length\n",
    "learn_totoal_steps = int(4e4) * episode_max_length\n",
    "verbose = 1\n",
    "batch_size = 1000\n",
    "tensorboard_log = '../tmp/ppo'\n",
    "callback_log_dir = '../tmp/paths'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_name = 'random-050-00272.tsp'\n",
    "target_cost = 9447\n",
    "instance = f'/home/fangbowen/LAHR/data/tsppdlib/instances/random-uniform/{instance_name}'\n",
    "locations = read_instance_data(instance)\n",
    "problem = MultiODProblem(locations=locations, ignore_to_dummy_cost=False)\n",
    "env = MultiODEnv(problem=problem, max_length=episode_max_length, max_time_length=episode_max_time_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dim = env.observation_space['solution'].shape[-1] + env.observation_space['problem'].shape[0]\n",
    "hidden_dim = 256\n",
    "num_heads = 16\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=PSExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=features_dim, \n",
    "                                   sol_input_dim=env.observation_space['solution'].shape[-1],\n",
    "                                   hidden_dim=hidden_dim,\n",
    "                                   num_heads=num_heads),\n",
    "    net_arch=dict(pi=[128, 128], vf=[128, 128]),\n",
    "    activation_fn=torch.nn.ReLU\n",
    ")\n",
    "model = PPO(\"MultiInputPolicy\", env, policy_kwargs=policy_kwargs, verbose=verbose, n_steps=n_steps, batch_size=batch_size, tensorboard_log=tensorboard_log)\n",
    "# model = DQN(\"MultiInputPolicy\", env, policy_kwargs=policy_kwargs, verbose=verbose, train_freq=n_steps, batch_size=batch_size, tensorboard_log=tensorboard_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_save_as = instance_name[:instance_name.index('.tsp')]\n",
    "model.learn(learn_totoal_steps, \n",
    "            tb_log_name=instance_save_as,\n",
    "            callback=SaveBestSolCallback(log_dir=callback_log_dir, \n",
    "                                         instance_name=instance_save_as, \n",
    "                                         verbose=verbose,\n",
    "                                         target_cost=target_cost)\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlor38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
