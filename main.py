import os 
from copy import deepcopy
import random 
import argparse
import numpy as np
import torch 
from stable_baselines3 import PPO, DQN, HerReplayBuffer
from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy
from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv
from stable_baselines3.common.vec_env.vec_monitor import VecMonitor

from src.env import MultiODEnv, SparseMultiODEnv, get_naive_action_dict, get_feasible_mapping_action_dict
from src import actions, operators
from src.solution import MultiODSolution
from src.problem import MultiODProblem
from src.utils import read_instance_data, get_lkh3_tour, get_ortools_tour
from src.rl.stable_baselines3.nn import PSExtractor
from src.rl.stable_baselines3.callback import SaveBestSolCallback


parser = argparse.ArgumentParser(description="Parameters for running proposed model.")
# Data
parser.add_argument('--instances', type=str, nargs='+', default=None, help="Instances to experiment on. ")
parser.add_argument('--instance_dir', type=str, default=os.path.join('data', 'tsppdlib', 'instances', 'random-uniform'), help="random-uniform instances directory.")
parser.add_argument('--num_O', type=int, default=5, help="number of O in instances.")
parser.add_argument('--resample_k', type=int, default=5, help="If resample, determines the number of instances.")
parser.add_argument('--lkh3_dir', type=str, default=os.path.join('..', 'U'), help="The LKH3 TOUR results for random-uniform instances. If not provided, will not be compared during training.")
parser.add_argument('--ortools_dir', type=str, default=os.path.join('..', 'tmp', 'ortools'), help="The Or-tools results for random-uniform instances. If not provided, will not be compared during training.")
# Params
parser.add_argument('--hidden_dim', type=int, default=256, help="Hidden dim of feature extractor.")
parser.add_argument('--num_heads', type=int, default=16, help="Number of heads in attention module of feature extractor.")
parser.add_argument('--learning_rate', '-lr', type=float, default=1e-3, help="Learning rate of Policy.")
parser.add_argument('--net_arch', type=int, nargs='+', default=[256, 256], help="List[int]. Number of layers and size of each layer.")
parser.add_argument('--batch_size', type=int, default=1000, help="Batch size for training.")
parser.add_argument('--n_gradient_steps', type=int, default=50, help="Number of epoch when optimizing the surrogate loss. ")
parser.add_argument('--n_steps', type=int, default=None, help="Number of steps per update.")
# Env
parser.add_argument('--episode_max_time_length', '-et', type=int, default=int(1e3), help="Max wall time for an episode.")
parser.add_argument('--episode_max_length', '-el', type=int, default=int(1e4), help="Max steps for an episode.")
parser.add_argument('--not_convert_distance_matrix_int', action='store_false', help="Whether to convert distance matrix to integer.")
parser.add_argument('--episode', type=int, default=500, help="Number of episodes.")
parser.add_argument('--k_recent', type=int, default=5, help="K recent actions and delta signs.")
parser.add_argument('--max_no_improvement', type=int, default=6, help="Max no cost improvement tolerance.")
parser.add_argument('--best_cost_tolerance', type=float, default=0.01, help="percentage to best cost tolerance.")
parser.add_argument('--nenv', type=int, default=1, help="Number of env for parallel running. ")
parser.add_argument('--seed', type=int, default=None, help="Seed for environment. For vec env, seeds are generated by +1 iteratively for envs.")
# Callback
parser.add_argument('--verbose', '-v', type=int, default=1, help="Verbose.")
parser.add_argument('--early_stop', action='store_false', help="Early stop when reaching the target cost.")
parser.add_argument('--tensorboard_log', type=str, default='../tmp/ppo', help='tensorboard logger directory.')
parser.add_argument('--callback_log_dir', type=str, default='../tmp/paths', help="Models and paths found storage directory.")
parser.add_argument('--env', choices=['multi-od', 'sparse-multi-od'], default='multi-od', help="RL environment. ")
# HER
parser.add_argument('--use_her', type=bool, default=False, help="For sparse reward environment, to use HER or not.")
parser.add_argument('--n_sampled_goal', type=int, default=4, help="Goal sample strategy for HER.")
parser.add_argument('--goal_selection_strategy', type=str, default='future', help="Goal sample strategy for HER.")
# Action dict
parser.add_argument('--action_dict', type=str, choices=['default', 'naive', 'feasible'], default='default', help="Whether to use naive operators, default, or feasible mapping.")


class MultiODEnvMaker:
    def __init__(self, problem, action_dict, max_length, max_time_length, k_recent, max_no_improvement, best_cost_tolerance, seed: int = None):
        self.problem = problem
        self.action_dict = action_dict
        self.max_length = max_length
        self.max_time_length = max_time_length
        self.k_recent = k_recent
        self.max_no_improvement = max_no_improvement
        self.best_cost_tolerance = best_cost_tolerance
        self.seed = seed

    def __call__(self):
        problem = deepcopy(self.problem)
        env = MultiODEnv(problem=problem, 
                         action_dict=self.action_dict,
                         max_length=self.max_length, 
                         max_time_length=self.max_time_length,
                         k_recent=self.k_recent,
                         max_no_improvement=self.max_no_improvement,
                         best_cost_tolerance=self.best_cost_tolerance,
                         seed=self.seed)
        return env


if __name__ == '__main__':
    args = parser.parse_args()
    if args.instances is not None:
        sub_instances = args.instances
    else:
        instances = [i for i in os.listdir(args.instance_dir) if i.endswith('.tsp')]
        num_O = f'{args.num_O:03}'
        sub_instances = [i for i in instances if '-' + num_O + '-' in i]
        sub_instances = random.sample(sub_instances, k=args.resample_k)
    
    if args.lkh3_dir == 'None':
        args.lkh3_dir = None 
    if args.ortools_dir == 'None':
        args.ortools_dir = None 
    if args.lkh3_dir is not None:
        lkh3_results = os.listdir(args.lkh3_dir)
    if args.ortools_dir is not None:
        ortools_results = os.listdir(args.ortools_dir)
    
    if args.action_dict == 'default':
        action_dict = None 
    elif args.action_dict == 'naive':
        action_dict = get_naive_action_dict
    elif args.action_dict == 'feasible':
        action_dict = get_feasible_mapping_action_dict

    for i in range(len(sub_instances)):
        instance_name = sub_instances[i]
        instance_name_head = instance_name[:instance_name.index('.tsp')]
        if args.verbose >= 1:
            print(f'instance: {instance_name_head}')
        instance =  os.path.join(args.instance_dir, instance_name)
        locations = read_instance_data(instance)
        problem = MultiODProblem(locations=locations, ignore_to_dummy_cost=False, ignore_from_dummy_cost=False)
        if args.not_convert_distance_matrix_int:
            problem.convert_distance_matrix_to_int()

        target_cost_str = ''
        if args.lkh3_dir is not None:
            lkh3_instance_result = [i for i in lkh3_results if instance_name_head in i][0]
            lkh3_tour = get_lkh3_tour(os.path.join(args.lkh3_dir, lkh3_instance_result))
            lkh3_tour = MultiODSolution([lkh3_tour], problem)
            lkh3_cost = problem.calc_cost(lkh3_tour)
            target_cost_str += f'LKH3 cost: {lkh3_cost} '
        if args.ortools_dir is not None:
            ortools_instance_result = [i for i in ortools_results if instance_name_head in i][0]
            ortools_tour = get_ortools_tour(os.path.join(args.ortools_dir, ortools_instance_result))
            ortools_tour = MultiODSolution([ortools_tour], problem)
            ortools_cost = problem.calc_cost(ortools_tour)
            target_cost_str += f'ortools cost: {ortools_cost}'
        if args.verbose >= 1:
            print(target_cost_str)
        
        if args.lkh3_dir is not None and args.ortools_dir is not None:
            if lkh3_cost < ortools_cost:
                target_tour = lkh3_tour  
                print('Target tour is LKH3')
            else:
                target_tour = ortools_tour
                print('Target tour is ortools')
        else:
            target_tour = None 
    
        if args.env == 'sparse-multi-od':
            env = SparseMultiODEnv(target_cost=int(problem.calc_cost(target_tour) * (1 + args.best_cost_tolerance)), 
                                problem=problem, 
                                action_dict=action_dict,
                                max_length=args.episode_max_length, 
                                max_time_length=args.episode_max_time_length,
                                k_recent=args.k_recent,
                                seed=args.seed)
        elif args.env == 'multi-od':
            env = VecMonitor(SubprocVecEnv([MultiODEnvMaker(problem, action_dict, args.episode_max_length, args.episode_max_time_length, args.k_recent, args.max_no_improvement, args.best_cost_tolerance, seed=(args.seed + i if args.seed is not None else args.seed)) for i in range(args.nenv)]))
            # env = MultiODEnv(problem=problem, action_dict=action_dict, max_length=episode_max_length, max_time_length=episode_max_time_length, k_recent=k_recent)
    
        features_dim = env.observation_space['solution'].shape[-1] + env.observation_space['problem'].shape[0]
    
        policy_kwargs = dict(
            features_extractor_class=PSExtractor,
            features_extractor_kwargs=dict(features_dim=features_dim, 
                                        sol_input_dim=env.observation_space['solution'].shape[-1],
                                        hidden_dim=args.hidden_dim,
                                        num_heads=args.num_heads),
            net_arch=args.net_arch
        )

        n_steps = args.n_steps if args.n_steps is not None else args.episode_max_length
        if args.use_her:
            replay_buffer_kwargs=dict(n_sampled_goal=args.n_sampled_goal, goal_selection_strategy=args.goal_selection_strategy)
            model = DQN("MultiInputPolicy", env, policy_kwargs=policy_kwargs, verbose=args.verbose,
                    train_freq=n_steps, batch_size=args.batch_size, tensorboard_log=args.tensorboard_log,
                    replay_buffer_class=HerReplayBuffer, replay_buffer_kwargs=replay_buffer_kwargs, seed=args.seed
                    )
        else:
            model = PPO("MultiInputPolicy", env, policy_kwargs=policy_kwargs, verbose=args.verbose, n_steps=n_steps, batch_size=args.batch_size, learning_rate=args.learning_rate, n_epochs=args.n_gradient_steps, tensorboard_log=args.tensorboard_log, seed=args.seed)
    
        instance_save_as = instance_name[:instance_name.index('.tsp')]
        learn_totoal_steps = args.episode * args.episode_max_length
        model.learn(learn_totoal_steps, 
                    tb_log_name=instance_save_as,
                    callback=SaveBestSolCallback(log_dir=args.callback_log_dir, 
                                                instance_name=instance_save_as, 
                                                verbose=args.verbose,
                                                target_tour=target_tour,
                                                early_stop=args.early_stop)
                    )