import os 
from copy import deepcopy
import random 
import argparse
import numpy as np
import torch 
from stable_baselines3 import PPO, DQN, HerReplayBuffer
from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy
from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv
from stable_baselines3.common.vec_env.vec_monitor import VecMonitor

from src.env import MultiODEnv, SparseMultiODEnv, PDPEnv
from src.action_dicts import *
from src import actions, operators
from src.solution import MultiODSolution
from src.problem import MultiODProblem, PDP
from src.utils import read_instance_data, get_lkh3_tour, get_ortools_tour, read_pdptw_instance_data
from src.rl.stable_baselines3.nn import PSExtractor
from src.rl.stable_baselines3.callback import SaveBestSolCallback


parser = argparse.ArgumentParser(description="Parameters for running proposed model.")
# Data
parser.add_argument('--instances', type=str, nargs='+', default=None, help="Instances to experiment on. ")
parser.add_argument('--instance_dir', type=str, default=os.path.join('data', 'tsppdlib', 'instances', 'random-uniform'), help="random-uniform instances directory.")
parser.add_argument('--num_O', type=int, default=5, help="number of O in instances.")
parser.add_argument('--resample_k', type=int, default=5, help="If resample, determines the number of instances.")
parser.add_argument('--lkh3_dir', type=str, default=os.path.join('..', 'U'), nargs='?', const=None, help="The LKH3 TOUR results for random-uniform instances. If not provided, will not be compared during training.")
parser.add_argument('--ortools_dir', type=str, default=os.path.join('..', 'tmp', 'ortools'), help="The Or-tools results for random-uniform instances. If not provided, will not be compared during training.")
# Params
parser.add_argument('--hidden_dim', type=int, default=256, help="Hidden dim of feature extractor.")
parser.add_argument('--num_heads', type=int, default=16, help="Number of heads in attention module of feature extractor.")
parser.add_argument('--learning_rate', '-lr', type=float, default=1e-3, help="Learning rate of Policy.")
parser.add_argument('--net_arch', type=int, nargs='+', default=[256, 256], help="List[int]. Number of layers and size of each layer.")
parser.add_argument('--batch_size', type=int, default=1000, help="Batch size for training.")
parser.add_argument('--n_gradient_steps', type=int, default=50, help="Number of epoch when optimizing the surrogate loss. ")
parser.add_argument('--n_steps', type=int, default=None, help="Number of steps per update.")
# Env
parser.add_argument('--episode_max_time_length', '-et', type=int, default=int(1e3), help="Max wall time for an episode.")
parser.add_argument('--episode_max_length', '-el', type=int, default=int(1e4), help="Max steps for an episode.")
parser.add_argument('--not_convert_distance_matrix_int', action='store_false', help="Whether to convert distance matrix to integer.")
parser.add_argument('--episode', type=int, default=500, help="Number of episodes.")
parser.add_argument('--k_recent', type=int, default=5, help="K recent actions and delta signs.")
parser.add_argument('--max_no_improvement', type=int, default=6, help="Max no cost improvement tolerance.")
parser.add_argument('--best_cost_tolerance', type=float, default=0.01, help="percentage to best cost tolerance.")
parser.add_argument('--nenv', type=int, default=1, help="Number of env for parallel running. ")
parser.add_argument('--seed', type=int, default=None, help="Seed for environment. For vec env, seeds are generated by +1 iteratively for envs.")
parser.add_argument('--capacity_slack', type=float, default=0.2, help='param for PDP, increase to the total capacity.')
parser.add_argument('--distance_type', type=str, choices=['EUC_2D', 'EXACT_2D'], default='EUC_2D')
# Callback
parser.add_argument('--verbose', '-v', type=int, default=1, help="Verbose.")
parser.add_argument('--early_stop', action='store_false', help="Early stop when reaching the target cost.")
parser.add_argument('--tensorboard_log', type=str, default='../tmp/ppo', help='tensorboard logger directory.')
parser.add_argument('--callback_log_dir', type=str, default='../tmp/paths', help="Models and paths found storage directory.")
parser.add_argument('--env', choices=['multi-od', 'sparse-multi-od', 'pdp'], default='multi-od', help="RL environment. ")
parser.add_argument('--method_name', type=str, default=None, help="name of result directory. ")
# HER
parser.add_argument('--use_her', type=bool, default=False, help="For sparse reward environment, to use HER or not.")
parser.add_argument('--n_sampled_goal', type=int, default=4, help="Goal sample strategy for HER.")
parser.add_argument('--goal_selection_strategy', type=str, default='future', help="Goal sample strategy for HER.")
# Action dict
parser.add_argument('--action_dict', type=str, choices=['default', 'naive', 'feasible', 'od_pair', 'intra_nxo', 'inter_nxo', 'same_bxo', 'mix_bxo'], default='default', help="Whether to use naive operators, default, or feasible mapping.")
parser.add_argument('--random_actions', type=str, choices=['default', 'od_pair', 'nxo', 'bxo'], default='default')


class EnvMaker:
    def __init__(self, cls, problem, action_dict, random_actions, max_length, max_time_length, k_recent, max_no_improvement, best_cost_tolerance, seed: int = None):
        self.cls = cls
        self.problem = problem
        self.action_dict = action_dict
        self.random_actions = random_actions
        self.max_length = max_length
        self.max_time_length = max_time_length
        self.k_recent = k_recent
        self.max_no_improvement = max_no_improvement
        self.best_cost_tolerance = best_cost_tolerance
        self.seed = seed

    def __call__(self):
        problem = deepcopy(self.problem)
        env = self.cls(problem=problem, 
                         action_dict=self.action_dict,
                         random_actions=self.random_actions,
                         max_length=self.max_length, 
                         max_time_length=self.max_time_length,
                         k_recent=self.k_recent,
                         max_no_improvement=self.max_no_improvement,
                         best_cost_tolerance=self.best_cost_tolerance,
                         seed=self.seed)
        return env


if __name__ == '__main__':
    args = parser.parse_args()
    if args.instances is not None:
        sub_instances = args.instances
    else:
        instances = [i for i in os.listdir(args.instance_dir) if i.endswith('.tsp') or i.endswith('.pdptw')]
        num_O = f'{args.num_O:03}'
        sub_instances = [i for i in instances if '-' + num_O + '-' in i]
        sub_instances = random.sample(sub_instances, k=args.resample_k)
    
    if args.lkh3_dir == 'None':
        args.lkh3_dir = None 
    if args.ortools_dir == 'None':
        args.ortools_dir = None 
    if args.lkh3_dir is not None:
        lkh3_results = os.listdir(args.lkh3_dir)
    if args.ortools_dir is not None:
        ortools_results = os.listdir(args.ortools_dir)
    
    if args.action_dict == 'default':
        action_dict = None 
    elif args.action_dict == 'naive':
        action_dict = get_naive_action_dict
    elif args.action_dict == 'feasible':
        action_dict = get_feasible_mapping_action_dict
    elif args.action_dict == 'od_pair':
        action_dict = get_od_pair_action_dict
    elif args.action_dict == 'intra_nxo':
        action_dict = get_intra_nxo_action_dict
    elif args.action_dict == 'inter_nxo':
        action_dict = get_inter_nxo_action_dict
    elif args.action_dict == 'same_bxo':
        action_dict = get_same_bxo_action_dict
    elif args.action_dict == 'mix_bxo':
        action_dict = get_mix_bxo_action_dict

    if args.random_actions == 'default':
        random_actions = None 
    elif args.random_actions == 'od_pair':
        random_actions = get_od_pair_random_actions()
    elif args.random_actions == 'nxo':
        random_actions = get_nxo_random_actions()
    elif args.random_actions == 'bxo':
        random_actions = get_bxo_random_actions()

    for i in range(len(sub_instances)):
        instance_name = sub_instances[i]
        _name = '.tsp' if args.env in ['multi-od', 'sparse-multi-od'] else '.pdptw' if args.env == 'pdp' else ''
        instance_name_head = instance_name[:instance_name.index(_name)] 
        if args.verbose >= 1:
            print(f'instance: {instance_name_head}')
        instance =  os.path.join(args.instance_dir, instance_name)

        if args.env in ['multi-od', 'sparse-multi-od']:
            locations = read_instance_data(instance)
            problem = MultiODProblem(locations=locations, ignore_to_dummy_cost=False, ignore_from_dummy_cost=False)
            if args.not_convert_distance_matrix_int:
                problem.convert_distance_matrix_to_int()
        elif args.env == 'pdp':
            locations, capacities, num_vehicles, capacity, distance_type = read_pdptw_instance_data(instance, capacity_slack=args.capacity_slack)
            problem = PDP(locations=locations, capacities=capacities, num_taxi=num_vehicles, capacity=capacity, distance_type=distance_type, ignore_from_dummy_cost=True, ignore_to_dummy_cost=True)

        target_cost_str = ''
        if args.lkh3_dir is not None:
            lkh3_instance_result = [i for i in lkh3_results if instance_name_head in i][0]
            lkh3_tour = get_lkh3_tour(os.path.join(args.lkh3_dir, lkh3_instance_result))
            lkh3_tour = MultiODSolution([lkh3_tour], problem)
            lkh3_cost = problem.calc_cost(lkh3_tour)
            target_cost_str += f'LKH3 cost: {lkh3_cost} '
        if args.ortools_dir is not None:
            if args.env in ['multi-od', 'sparse-multi-od']:
                num_vehicles = 1
                include_taxi_node = True 
                skip_first_lines = 2
            elif args.env == 'pdp':
                include_taxi_node = False 
                skip_first_lines = 1
            ortools_instance_result = [i for i in ortools_results if instance_name_head in i][0]
            ortools_tour = get_ortools_tour(os.path.join(args.ortools_dir, ortools_instance_result), num_taxi=num_vehicles, include_taxi_node=include_taxi_node, skip_first_lines=skip_first_lines)
            ortools_tour = MultiODSolution(ortools_tour, problem)
            ortools_cost = problem.calc_cost(ortools_tour)
            target_cost_str += f'ortools cost: {ortools_cost}'
        if args.verbose >= 1:
            print(target_cost_str)
        
        if args.lkh3_dir is not None and args.ortools_dir is not None:
            if lkh3_cost < ortools_cost:
                target_tour = lkh3_tour  
                print('Target tour is LKH3')
            else:
                target_tour = ortools_tour
                print('Target tour is ortools')
        elif args.lkh3_dir is not None:
            target_tour = lkh3_tour 
            print('Target tour is LKH3')
        elif args.ortools_dir is not None:
            target_tour = ortools_tour
            print('Target tour is ortools')
        else:
            target_tour = None
    
        if args.env == 'sparse-multi-od':
            env = SparseMultiODEnv(target_cost=int(problem.calc_cost(target_tour) * (1 + args.best_cost_tolerance)), 
                                problem=problem, 
                                action_dict=action_dict,
                                max_length=args.episode_max_length, 
                                max_time_length=args.episode_max_time_length,
                                k_recent=args.k_recent,
                                seed=args.seed,
                                random_actions=random_actions)
        elif args.env == 'multi-od':
            env = VecMonitor(SubprocVecEnv([EnvMaker(MultiODEnv, problem, action_dict, random_actions, args.episode_max_length, args.episode_max_time_length, args.k_recent, args.max_no_improvement, args.best_cost_tolerance, seed=(args.seed + i if args.seed is not None else args.seed)) for i in range(args.nenv)]))
            # env = MultiODEnv(problem=problem, action_dict=action_dict, max_length=episode_max_length, max_time_length=episode_max_time_length, k_recent=k_recent)
        elif args.env == 'pdp':
            env = VecMonitor(SubprocVecEnv([EnvMaker(PDPEnv, problem, action_dict, random_actions, args.episode_max_length, args.episode_max_time_length, args.k_recent, args.max_no_improvement, args.best_cost_tolerance, seed=(args.seed + i if args.seed is not None else args.seed)) for i in range(args.nenv)]))
    
        features_dim = env.observation_space['solution'].shape[-1] + env.observation_space['problem'].shape[0]
    
        policy_kwargs = dict(
            features_extractor_class=PSExtractor,
            features_extractor_kwargs=dict(features_dim=features_dim, 
                                        sol_input_dim=env.observation_space['solution'].shape[-1],
                                        hidden_dim=args.hidden_dim,
                                        num_heads=args.num_heads),
            net_arch=args.net_arch
        )

        n_steps = args.n_steps if args.n_steps is not None else args.episode_max_length
        if args.use_her:
            replay_buffer_kwargs=dict(n_sampled_goal=args.n_sampled_goal, goal_selection_strategy=args.goal_selection_strategy)
            model = DQN("MultiInputPolicy", env, policy_kwargs=policy_kwargs, verbose=args.verbose,
                    train_freq=n_steps, batch_size=args.batch_size, tensorboard_log=args.tensorboard_log,
                    replay_buffer_class=HerReplayBuffer, replay_buffer_kwargs=replay_buffer_kwargs, seed=args.seed
                    )
        else:
            model = PPO("MultiInputPolicy", env, policy_kwargs=policy_kwargs, verbose=args.verbose, n_steps=n_steps, batch_size=args.batch_size, learning_rate=args.learning_rate, n_epochs=args.n_gradient_steps, tensorboard_log=args.tensorboard_log, seed=args.seed)
    
        instance_save_as = f'{instance_name[:instance_name.index(_name)]}_{args.method_name if args.method_name is not None else ""}'
        learn_totoal_steps = args.episode * args.episode_max_length
        model.learn(learn_totoal_steps, 
                    tb_log_name=instance_save_as,
                    callback=SaveBestSolCallback(log_dir=args.callback_log_dir, 
                                                instance_name=instance_save_as, 
                                                verbose=args.verbose,
                                                target_tour=target_tour,
                                                early_stop=args.early_stop,
                                                distance_type=args.distance_type)
                    )